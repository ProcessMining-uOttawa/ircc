{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook for new ircc dataset (combined_event_log_anonymous.csv)\n",
    "\n",
    "# TODO\n",
    "# âˆš mark subprocesses that are fully automated; incorporate in viewer\n",
    "# in general, nesting subprocesses in dcr (paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/combined_event_log_anonymous.csv\n",
    "# -> data/combined_event_log-filt_evt1p.csv\n",
    "#   -> data/combined_event_log-filt_evt1p-time1m.csv\n",
    "\n",
    "# subprocesses:\n",
    "#   data/combined_event_log-filt_evt1p-time1m.csv\n",
    "#   -> data/combined_event_log-abstracted.csv + sublogs in level2 (based on status) \n",
    "#       -> data/combined_event_log-abstracted2.csv + sublogs in level1 (based on nesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mine_utils import get_log\n",
    "\n",
    "# pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huge amount of variability\n",
    "\n",
    "# total # traces = 7734 traces\n",
    "\n",
    "# (non-abstr) new log: # vars = 7665, ratio = 99.10783553141971\n",
    "#   (a variant covers at most 4 traces ...)\n",
    "#   filter evts in less than X of cases:\n",
    "#       1%: 7664 variants: minus 1 variant\n",
    "#       10%: 7657 variants: minus 8 variants\n",
    "#   same time for events within 1min:\n",
    "#       ** 6030 variants: minus 1635 variants\n",
    "#       + above (1%): 6014 variants: minus 1651 variants\n",
    "#       + above (10%): 5841 variants: minus 1824 variants\n",
    "\n",
    "# abstr new log: # vars = 5145, ratio = 66.5244375484872\n",
    "#   same time for events within 1min: # vars = 3654, ratio = 47.25\n",
    "#   + filter evts in less than 1% of cases: # vars = 3564, ratio = 46.08\n",
    "\n",
    "# (abstr or log: # traces = 7734, # vars = 2642, ratio = 34.16084820274114)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variant_stats import get_variants_stats, get_variant_ratio, get_variant_coverage, get_covering_variants, filter_traces_on_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log-filt_evt1p.csv\")\n",
    "vars_log = get_variants_stats(log)\n",
    "print(get_variant_ratio(log, vars_log))\n",
    "vars_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much bigger improvement, clearly\n",
    "\n",
    "log = pd.read_csv(\"data/combined_event_log-time1m.csv\")\n",
    "vars_log = get_variants_stats(log)\n",
    "print(get_variant_ratio(log, vars_log))\n",
    "vars_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log-filt_evt1p-time1m.csv\")\n",
    "vars_log = get_variants_stats(log)\n",
    "print(get_variant_ratio(log, vars_log))\n",
    "vars_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")\n",
    "log['act_upd_date'] = pd.to_datetime(log['act_upd_date'])\n",
    "log = log.rename({ 'app_num': 'case:concept:name', 'activity_full_value': 'concept:name', 'act_upd_date': 'time:timestamp' }, axis=1)\n",
    "\n",
    "abstr_log_or = pd.read_csv(\"data/or/abstract_log-starts_ends-v2.csv\")\n",
    "abstr_log_new = pd.read_csv(\"data/combined_event_log-abstracted2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total # traces:\", len(log['case:concept:name'].unique()))\n",
    "\n",
    "vars_log = get_variants_stats(log)\n",
    "vars_abstr_log_or = get_variants_stats(abstr_log_or)\n",
    "vars_abstr_log_new = get_variants_stats(abstr_log_new)\n",
    "\n",
    "print(\"log:\", get_variant_ratio(log, vars_log))\n",
    "print(\"abstr log or:\", get_variant_ratio(log, vars_abstr_log_or))\n",
    "print(\"abstr log new:\", get_variant_ratio(log, vars_abstr_log_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrequent events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")\n",
    "log = log.rename({ 'app_num': 'case:concept:name', 'activity_full_value': 'concept:name', 'act_upd_date': 'time:timestamp' }, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[log['activity']=='Other Reqs Assessment']['activity_status'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from log_stats import count_cases_per_event\n",
    "\n",
    "activ_cases_counts = count_cases_per_event('concept:name', 'case:concept:name', log).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activ_cases_counts[activ_cases_counts['perc']<1] #.to_csv(\"data/dropped_events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activ_cases_counts[activ_cases_counts['perc']<10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's filter on activities that occur in 1% or less of cases\n",
    "to_drop = activ_cases_counts.loc[activ_cases_counts['perc']<1, 'concept:name']\n",
    "log_filter = log[~ log['concept:name'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filter.to_csv(\"data/combined_event_log-filt_evt1p.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")\n",
    "\n",
    "log = pd.read_csv(\"data/combined_event_log-filt_evt1p.csv\")\n",
    "log = log.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "log = log.rename({ 'app_num': 'case:concept:name', 'activity_full_value': 'concept:name', 'act_upd_date': 'time:timestamp' }, axis=1)\n",
    "log['time:timestamp'] = pd.to_datetime(log['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine_utils import get_time_diff, equal_timestamps_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_time_diff(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[log['activity']=='Associate Medicals'].groupby('case:concept:name')['time_diff'].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (check all sequential events with exact same timestamp)\n",
    "print(\"num simult (0 sec):\", len(log[log['time_diff']==0]), \" <> total num:\", log.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (check all sequential events with less than 1min difference in timestamps)\n",
    "print(\"num simult (1 min):\", len(log[log['time_diff'] < 60]), \" <> total num:\", log.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (takes ca. 2-4 sec)\n",
    "\n",
    "log = equal_timestamps_interval(log, 60) # 60 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log.to_csv(\"data/combined_event_log-time1m.csv\")\n",
    "log.to_csv(\"data/combined_event_log-filt_evt1p-time1m.csv\")\n",
    "\n",
    "# checkout variant analysis for seeing whether it improved matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = \"data/combined_event_log-filt_evt1p.csv\"\n",
    "# src_file = \"data/combined_event_log-filt_evt1p-time1m.csv\"\n",
    "\n",
    "tgt_folder = \"lifecycles/filt_evt1p/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subprocesses based on status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")\n",
    "# log = log.rename({ 'app_num': 'case:concept:name', 'act_upd_date': 'time:timestamp' }, axis=1)\n",
    "\n",
    "log = pd.read_csv(src_file)\n",
    "log = log.drop('Unnamed: 0', axis=1)\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique activities\n",
    "log['activity'].drop_duplicates().sort_values().to_excel(\"data/all_activities.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activity subprocesses\n",
    "activ_lifecycles = log[['activity', 'activity_status']].drop_duplicates().sort_values(by=['activity'])\n",
    "activ_lifecycles.to_excel(\"data/activity_lifecycles.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of sub-activities in each subprocess\n",
    "counts = log[['activity', 'activity_status']].drop_duplicates().groupby('activity')['activity_status'].count()\n",
    "counts = counts.sort_values(ascending=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only separate subprocesses with >= 3 sub-activities\n",
    "parent_activ = counts[counts >= 3].reset_index()\n",
    "parent_activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subproc_evts = log[log['activity'].isin(parent_activ['activity'])]\n",
    "subproc_evts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_subproc_evts = log[~ log['activity'].isin(parent_activ['activity'])]\n",
    "non_subproc_evts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create separate logs per subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from separ_subproc import separ_subproc\n",
    "\n",
    "separ_subproc(subproc_evts, non_subproc_evts, 'activity', 'activity_status', 'concept:name', f\"{tgt_folder}/level2/\", f\"{tgt_folder}/combined_event_log-abstracted_status.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subprocesses based on nesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check original nesting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestings = pd.read_excel(\"data/nested_activities-original.xlsx\")\n",
    "log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestings[~ (nestings['Activity'].isin(log['activity']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_recov = nestings[nestings['Activity']=='Cost Recovery']['Parent Item']\n",
    "cost_recov[~ (cost_recov.isin(log['activity']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(log[log['activity']=='Criminality Assessment']))\n",
    "print(len(log[log['activity']=='Medical Assessment']))\n",
    "print(len(log[log['activity']=='Misrep Assessment']))\n",
    "print(len(log[log['activity']=='Security Assessment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log[log['activity'].str.startswith('Biographic')]['activity'].drop_duplicates())\n",
    "print()\n",
    "print(log[log['activity'].str.startswith('Biometric')]['activity'].drop_duplicates())\n",
    "print()\n",
    "print(log[log['activity'].str.startswith('Criminal')]['activity'].drop_duplicates())\n",
    "print()\n",
    "print(log[log['activity'].str.startswith('Medical')]['activity'].drop_duplicates())\n",
    "print()\n",
    "print(log[log['activity'].str.startswith('Security')]['activity'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestings = pd.read_excel(\"data/nested_activities-fixed.xlsx\")\n",
    "nestings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there's no non-existent activities in the nesting file\n",
    "or_log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")\n",
    "nestings[~ nestings['Activity'].isin(or_log['activity'])].sort_values(by='Activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstr_log = pd.read_csv(f\"{tgt_folder}/combined_event_log-abstracted_status.csv\")\n",
    "abstr_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect parent items to events\n",
    "# left merge; also keep events that are not being nested\n",
    "abstr_log_parent = abstr_log.merge(nestings, left_on='activity', right_on='Activity', how='left')\n",
    "abstr_log_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subproc_evts = abstr_log_parent[abstr_log_parent['Parent Item'].notna()]\n",
    "# (non-nested events; those not merged with parent)\n",
    "non_subproc_evts = abstr_log_parent[abstr_log_parent['Parent Item'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create separate logs per nested activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from separ_subproc import separ_subproc\n",
    "from shutil import copy\n",
    "\n",
    "separ_subproc(subproc_evts, non_subproc_evts, 'Parent Item', 'concept:name', 'concept:name', f\"{tgt_folder}/level1\", f\"{tgt_folder}/combined_event_log-abstracted_nesting.csv\")\n",
    "\n",
    "copy(f\"{tgt_folder}/combined_event_log-abstracted_nesting.csv\", f\"{tgt_folder}/level0/main.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir = \"lifecycles/level2/logs\"\n",
    "sublogs_lvl2 = [ (f, pd.read_csv(os.path.join(dir, f))) for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check original events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same input as subprocesses part\n",
    "# (NOTE - original log will have to be extended with index for sanity code to work)\n",
    "log = pd.read_csv(\"data/combined_event_log-filt_evt1p-time1m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "found_indexes = set()\n",
    "\n",
    "# > subprocesses based on status\n",
    "\n",
    "# - find original events in the sublogs\n",
    "\n",
    "for f, sublog in sublogs_lvl2:    \n",
    "    f_name = f[0:f.index(\".\")]\n",
    "    \n",
    "    # join original log with the sublog\n",
    "    log_merged = log.merge(sublog, left_on='index', right_on='index')\n",
    "    # print(log_merged)\n",
    "    \n",
    "    # (apply same string operation as on the file name)\n",
    "    activ_file = (log_merged['activity_x'].str.replace(\"/\", \"_\") == f_name)\n",
    "    # for all matches, activity names should correspond to file name\n",
    "    assert (activ_file).all(), f_name + \" <> \" + log_merged[~ activ_file]['activity_x']\n",
    "    # for all matches, activity statuses should correspond to sublog activity name\n",
    "    assert (log_merged['activity_status'] == log_merged['concept:name_y']).all(), \"file: {f_name}\"\n",
    "    \n",
    "    found_indexes.update(log_merged['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - find original events in the abstracted log\n",
    "\n",
    "# only interested in activities without parent (non-nested)\n",
    "log_filter = log[~ log['activity'].isin(parent_activ['activity'])]\n",
    "# join original log with the abstracted log\n",
    "log_merged = log_filter.merge(abstr_log, left_on='index', right_on='index')\n",
    "# print(log_merged)\n",
    "\n",
    "activ_name = (log_merged['concept:name_x'] == log_merged['concept:name_y'])\n",
    "# activity names should correspond\n",
    "assert (activ_name).all(), log_merged[~ activ_name][['concept:name_x', 'concept:name_y']]\n",
    "\n",
    "found_indexes.update(log_merged['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all events found?\n",
    "assert len(found_indexes) == log.shape[0], f\"{len(found_indexes)} <> {log.shape[0]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > subprocesses based on nesting\n",
    "\n",
    "# per case, check if activity's start/end markers correspond to first & last sorted events of the activity\n",
    "\n",
    "def check_subproc(abstr_log, marker_label, is_start):\n",
    "    # group all start/end markers by activity\n",
    "    groups = abstr_log[abstr_log['concept:name'].str.endswith(marker_label)].groupby('activity')\n",
    "\n",
    "    # for each activity & their markers for all cases\n",
    "    for activ, g in groups:\n",
    "        # sort on case\n",
    "        g = g.sort_values(by='case:concept:name').reset_index()\n",
    "        \n",
    "        # find sublog corresponding to activity\n",
    "        for i in [ 1, 2 ]:\n",
    "            path = f\"lifecycles/level2/logs/{activ.replace('/', '_')}.csv\"\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "            sublog = pd.read_csv(path)\n",
    "        \n",
    "        # print(activ, path)\n",
    "        \n",
    "        # first, in the activity's sublog, find the firsts/lasts for each case\n",
    "        gb = sublog.groupby('case:concept:name')\n",
    "        delims = (gb.first() if is_start else gb.last()).reset_index()\n",
    "        # also sort on case\n",
    "        delims = delims.sort_values(by='case:concept:name').reset_index()\n",
    "        \n",
    "        # firsts/lasts should be the same as the start/end markers\n",
    "        assert(g['index'] == delims['index']).all(), activ\n",
    "        \n",
    "check_subproc(abstr_log, \" [begin]\", True)\n",
    "check_subproc(abstr_log, \" [end]\", False)\n",
    "\n",
    "path = \"lifecycles/level1/logs/\"\n",
    "\n",
    "from pathlib import Path\n",
    "paths = Path(path).rglob(\"*.csv\")\n",
    "for path in paths:\n",
    "    abstr_sublog = pd.read_csv(path)\n",
    "    check_subproc(abstr_sublog, \" [begin]\", True)\n",
    "    check_subproc(abstr_sublog, \" [end]\", False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant analysis (bis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main variant (no filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variant_stats import get_variants_stats, get_variant_ratio, get_variant_coverage, get_covering_variants, filter_traces_on_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_log(\"data/combined_event_log-abstracted2.csv\")\n",
    "log = log[['case:concept:name', 'concept:name', 'time:timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_stats = get_variants_stats(log)\n",
    "print(get_variant_ratio(log, var_stats))\n",
    "var_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton_vars = var_stats[var_stats['cov_amt']==1]\n",
    "\n",
    "# ooph, a lot of traces here ...\n",
    "singleton_vars['cov_amt'].sum() / var_stats['cov_amt'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subprocess variants (filtering!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some ad-hoc analysis\n",
    "\n",
    "# log = pd.read_csv(\"/Users/wvw/git/pm/ircc/lifecycles/level1/logs/Other Req Activity.csv\")\n",
    "# var_stats = get_variants_stats(log, plot=False)\n",
    "# print(var_stats)\n",
    "# rare_vars = var_stats[var_stats['cov_perc'] < 1]\n",
    "# print()\n",
    "# print(rare_vars['cov_perc'].sum().round(2))\n",
    "# print((rare_vars['sequence'].count() / var_stats.shape[0] * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the variants\n",
    "# (ca. 3-5 s)\n",
    "\n",
    "dir = \"/Users/wvw/git/pm/ircc/lifecycles\"\n",
    "for subdir in [ \"level1\", \"level2\" ]:\n",
    "    print(\">\", subdir)\n",
    "    for path in Path(os.path.join(dir, subdir, \"logs\")).rglob(\"*.csv\"):\n",
    "        name = os.path.basename(path)\n",
    "        log = pd.read_csv(path)\n",
    "        var_stats = get_variants_stats(log, plot=False)\n",
    "        \n",
    "        rare_vars = var_stats[var_stats['cov_perc'] < 1]\n",
    "        print(name, \":\", rare_vars['cov_perc'].sum().round(2), \"% traces\", \"(\", (rare_vars['sequence'].count() / var_stats.shape[0] * 100).round(2), \"% vars\" \")\")\n",
    "        \n",
    "        flog = filter_traces_on_variants(log, var_stats[var_stats['cov_perc'] >= 1])\n",
    "        flog.to_csv(path)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Timestamp differences (bis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_log(\"data/combined_event_log-abstracted2.csv\")\n",
    "log = log[['case:concept:name', 'concept:name', 'time:timestamp']]\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine_utils import get_time_diff\n",
    "\n",
    "logd = get_time_diff(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (check all sequential events with less than 1min difference in timestamps)\n",
    "print(\"num simult (1 min):\", len(logd[ (logd['time_diff'] > 0) & (logd['time_diff'] < 60)]), \" <> total num:\", logd.shape[0])\n",
    "\n",
    "# none to be found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Infrequent events (bis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log-abstracted2.csv\")\n",
    "\n",
    "from log_stats import count_cases_per_event\n",
    "activ_cases_counts = count_cases_per_event('concept:name', 'case:concept:name', log).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activ_cases_counts[activ_cases_counts['perc']<1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine process models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ad-hoc mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variant_stats import get_variants_stats, get_variant_ratio, get_variant_coverage, get_covering_variants, filter_traces_on_variants\n",
    "from mine_utils import get_log, ProcAnn, mine_heur, mine_induct, mine_alpha, mine_dfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_log(\"data/combined_event_log-abstracted2.csv\")\n",
    "\n",
    "cost_recov = log[(log['concept:name']=='Cost Recovery [begin]') | (log['concept:name']=='Cost Recovery [end]')]\n",
    "\n",
    "log = log[['case:concept:name', 'time:timestamp', 'concept:name']]\n",
    "cost_recov.to_csv(\"data/cost_recov.csv\")\n",
    "xes_export.apply(cost_recov, \"data/cost_recov.xes\")\n",
    "\n",
    "mine_heur(log, ProcAnn.FREQ, \"graphs/combined_event_log-abstracted2-time1m\")\n",
    "mine_induct(log, convert_to='petri_net', output_path=\"graphs/combined_event_log-abstracted2-time1m-pn\")\n",
    "\n",
    "log = log[['case:concept:name', 'time:timestamp', 'concept:name']]\n",
    "xes_export.apply(log, \"data/combined_event_log-abstracted2.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif = get_log(\"/Users/wvw/git/pm/ircc/lifecycles/level2/logs/Verification.csv\")\n",
    "verif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = get_variants_stats(verif)\n",
    "vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_dfg(verif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_folder = \"lifecycles/filt_evt1p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine_utils import ProcAnn, mine_heur, mine_induct, mine_alpha, mine_dfg\n",
    "import pm4py.objects.log.exporter.xes.exporter as xes_export\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "def init_subdir(subdir, subsubdirs=[]):\n",
    "    if os.path.exists(subdir):\n",
    "        shutil.rmtree(subdir)\n",
    "    os.mkdir(subdir)\n",
    "    for subsubdir in subsubdirs:\n",
    "        os.mkdir(os.path.join(subdir, subsubdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "induct_formats = ['bpmn', 'petri_net']\n",
    "formats_with_ann = ['dfg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_entries_json(names, default_format, default_ann, path):\n",
    "    def entry_pref():\n",
    "        if default_format in formats_with_ann:\n",
    "            return f\"{{ \\\"format\\\": \\\"{default_format}\\\", \\\"ann\\\": \\\"{default_ann.value}\\\" }}\"\n",
    "        else:\n",
    "            return f\"{{ \\\"format\\\": \\\"{default_format}\\\" }}\"\n",
    "    \n",
    "    all = \"[\" + \", \".join(map(lambda n: f\"\\\"{n}\\\"\", names)) + \"]\"\n",
    "    prefs = \"{\" + \", \".join(map(lambda n: f\"\\\"{n}\\\": {entry_pref()}\", names)) + \"}\"\n",
    "    obj = f\"{{ \\\"all\\\": {all}, \\\"prefs\\\": {prefs} }}\"\n",
    "    open(os.path.join(path, \"graphs.json\"), \"w\").write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine process models for sublogs\n",
    "# (ca. 1 min)\n",
    "\n",
    "for lvl in [ 1, 2 ]:\n",
    "    # subprocess level (levels 1-2)\n",
    "    subdir = os.path.join(tgt_folder, f\"level{lvl}\")\n",
    "\n",
    "    # default model & annotation to be shown\n",
    "    default_format = \"bpmn\"\n",
    "    default_ann = ProcAnn.FREQ\n",
    "\n",
    "    init_subdir(os.path.join(subdir, \"xes\"))\n",
    "    init_subdir(os.path.join(subdir, \"dfg\"), [ ProcAnn.FREQ.value, ProcAnn.PERF.value ])\n",
    "    init_subdir(os.path.join(subdir, \"heur\"), [ ProcAnn.FREQ.value, ProcAnn.PERF.value ])\n",
    "    for format in induct_formats:\n",
    "        init_subdir(os.path.join(subdir, format))\n",
    "\n",
    "    names = [ ]\n",
    "    for path in Path(os.path.join(tgt_folder, \"data\", f\"level{lvl}\")).rglob(\"*.csv\"):\n",
    "        file = os.path.basename(path)\n",
    "        name = file[0: file.index(\".csv\")]\n",
    "        print(name)\n",
    "        names.append(name)\n",
    "        \n",
    "        log = pd.read_csv(path)\n",
    "        log = log[['case:concept:name', 'time:timestamp', 'concept:name']]\n",
    "        \n",
    "        log['case:concept:name'] = log['case:concept:name'].astype('int64')\n",
    "        log['time:timestamp'] = pd.to_datetime(log['time:timestamp'])\n",
    "        \n",
    "        xes_export.apply(log, os.path.join(subdir, \"xes\", name + \".xes\"))\n",
    "        \n",
    "        for ann in ProcAnn:\n",
    "            mine_dfg(log, ann, output_path=os.path.join(subdir, \"dfg\", ann.value, name), save_gviz=True)\n",
    "            mine_heur(log, ann, output_path=os.path.join(subdir, \"heur\", ann.value, name), save_gviz=True)\n",
    "        \n",
    "        for format in induct_formats:\n",
    "            mine_induct(log, convert_to=format, output_path=os.path.join(subdir, format, name), save_gviz=True)\n",
    "\n",
    "    save_entries_json(names, default_format, default_ann, subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine models for main process\n",
    "# (ca. 30-40 sec)\n",
    "\n",
    "log = pd.read_csv(os.path.join(tgt_folder, \"data\", \"level0\", \"main.csv\"))\n",
    "log = log[['case:concept:name', 'time:timestamp', 'concept:name']]\n",
    "log['time:timestamp'] = pd.to_datetime(log['time:timestamp'])\n",
    "\n",
    "subdir = os.path.join(tgt_folder, \"level0\")\n",
    "\n",
    "default_format = \"dcr\"\n",
    "default_ann = ProcAnn.FREQ\n",
    "\n",
    "init_subdir(os.path.join(subdir, \"logs\"))\n",
    "init_subdir(os.path.join(subdir, \"dfg\"), [ ProcAnn.FREQ.value, ProcAnn.PERF.value ])\n",
    "init_subdir(os.path.join(subdir, \"heur\"), [ ProcAnn.FREQ.value, ProcAnn.PERF.value ])\n",
    "init_subdir(os.path.join(subdir, \"bpmn\"))\n",
    "init_subdir(os.path.join(subdir, \"petri_net\"))\n",
    "init_subdir(os.path.join(subdir, \"xes\"))\n",
    "\n",
    "name = \"main\"\n",
    "log.to_csv(os.path.join(subdir, \"logs\", name + \".csv\"))\n",
    "\n",
    "for ann in ProcAnn:\n",
    "    mine_dfg(log, ann, output_path=os.path.join(subdir, \"dfg\", ann.value, name), save_gviz=True)\n",
    "    mine_heur(log, ann, output_path=os.path.join(subdir, \"heur\", ann.value, name), save_gviz=True)\n",
    "\n",
    "for format in induct_formats:\n",
    "    mine_induct(log, convert_to=format, output_path=os.path.join(subdir, format, name), save_gviz=True)\n",
    "\n",
    "xes_export.apply(log, os.path.join(subdir, \"xes\", name + \".xes\"))\n",
    "\n",
    "save_entries_json([name], default_format, default_ann, subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine DCR: see\n",
    "# /Users/wvw/git/pm/declarative/dcr4py/pm4py-dcr/ircc_dcr.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

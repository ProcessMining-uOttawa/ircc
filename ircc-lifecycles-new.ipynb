{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook for new ircc dataset (combined_event_log_anonymous.csv)\n",
    "\n",
    "# TODO\n",
    "# âˆš mark subprocesses that are fully automated; incorporate in viewer\n",
    "# in general, nesting subprocesses in dcr (paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/combined_event_log_anonymous.csv\n",
    "# -> data/combined_event_log-filt_evt1p.csv\n",
    "#   -> data/combined_event_log-filt_evt1p-time1m.csv\n",
    "\n",
    "# subprocesses:\n",
    "#   data/combined_event_log-filt_evt1p-time1m.csv\n",
    "#   -> data/combined_event_log-abstracted.csv + sublogs in level2 (based on status) \n",
    "#       -> data/combined_event_log-abstracted2.csv + sublogs in level1 (based on nesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mine_utils import get_log\n",
    "\n",
    "# pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter unknown activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_filter = pd.read_csv(\"data/unknown_activ.csv\")\n",
    "to_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")\n",
    "flog = log[~ log['activity'].isin(to_filter['Activity'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log.shape[0], \"-->\", flog.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flog.to_csv(\"data/log-filt_unkn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter fee-related activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/log-filt_unkn.csv\", )\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestings = pd.read_csv(\"data/nested_activities-17_10_25.csv\")\n",
    "cost_activ = nestings[nestings['Parent Item']=='Cost Recovery']\n",
    "cost_activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flog = log[~log['activity'].isin(cost_activ['Activity'])]\n",
    "# sanity check\n",
    "flog[flog['activity'].str.contains(\"Fee\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log.shape[0], \"-->\", flog.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flog.to_csv(\"data/log-filt_unkn-filt_fees.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huge amount of variability\n",
    "\n",
    "# total # traces = 7734 traces\n",
    "\n",
    "# (non-abstr) new log: # vars = 7665, ratio = 99.10783553141971\n",
    "#   (a variant covers at most 4 traces ...)\n",
    "#   filter evts in less than X of cases:\n",
    "#       1%: 7664 variants: minus 1 variant\n",
    "#       10%: 7657 variants: minus 8 variants\n",
    "#   same time for events within 1min:\n",
    "#       ** 6030 variants: minus 1635 variants\n",
    "#       + above (1%): 6014 variants: minus 1651 variants\n",
    "#       + above (10%): 5841 variants: minus 1824 variants\n",
    "\n",
    "# abstr new log: # vars = 5145, ratio = 66.5244375484872\n",
    "#   same time for events within 1min: # vars = 3654, ratio = 47.25\n",
    "#   + filter evts in less than 1% of cases: # vars = 3564, ratio = 46.08\n",
    "\n",
    "# (abstr or log: # traces = 7734, # vars = 2642, ratio = 34.16084820274114)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variant_stats import get_variants_stats, get_variant_ratio, get_variant_coverage, get_covering_variants, filter_traces_on_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log-filt_evt1p.csv\")\n",
    "vars_log = get_variants_stats(log)\n",
    "print(get_variant_ratio(log, vars_log))\n",
    "vars_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much bigger improvement, clearly\n",
    "\n",
    "log = pd.read_csv(\"data/combined_event_log-time1m.csv\")\n",
    "vars_log = get_variants_stats(log)\n",
    "print(get_variant_ratio(log, vars_log))\n",
    "vars_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log-filt_evt1p-time1m.csv\")\n",
    "vars_log = get_variants_stats(log)\n",
    "print(get_variant_ratio(log, vars_log))\n",
    "vars_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")\n",
    "log['act_upd_date'] = pd.to_datetime(log['act_upd_date'])\n",
    "\n",
    "abstr_log_or = pd.read_csv(\"data/or/abstract_log-starts_ends-v2.csv\")\n",
    "abstr_log_new = pd.read_csv(\"data/combined_event_log-abstracted2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total # traces:\", len(log['case:concept:name'].unique()))\n",
    "\n",
    "vars_log = get_variants_stats(log)\n",
    "vars_abstr_log_or = get_variants_stats(abstr_log_or)\n",
    "vars_abstr_log_new = get_variants_stats(abstr_log_new)\n",
    "\n",
    "print(\"log:\", get_variant_ratio(log, vars_log))\n",
    "print(\"abstr log or:\", get_variant_ratio(log, vars_abstr_log_or))\n",
    "print(\"abstr log new:\", get_variant_ratio(log, vars_abstr_log_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrequent events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[log['activity']=='Other Reqs Assessment']['activity_status'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from log_stats import count_cases_per_event\n",
    "\n",
    "activ_cases_counts = count_cases_per_event('concept:name', 'case:concept:name', log).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activ_cases_counts[activ_cases_counts['perc']<1].to_csv(\"data/infreq_evts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's filter on activities that occur in 1% or less of cases\n",
    "to_drop = activ_cases_counts.loc[activ_cases_counts['perc']<1, 'concept:name']\n",
    "log_filter = log[~ log['concept:name'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filter.to_csv(\"data/combined_event_log-filt_evt1p.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = pd.read_csv(\"data/combined_event_log_anonymous.csv\")\n",
    "\n",
    "log = pd.read_csv(\"data/combined_event_log-filt_evt1p.csv\")\n",
    "log['time:timestamp'] = pd.to_datetime(log['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine_utils import get_time_diff, equal_timestamps_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_time_diff(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[log['activity']=='Associate Medicals'].groupby('case:concept:name')['time_diff'].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (check all sequential events with exact same timestamp)\n",
    "print(\"num simult (0 sec):\", len(log[log['time_diff']==0]), \" <> total num:\", log.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (check all sequential events with less than 1min difference in timestamps)\n",
    "print(\"num simult (1 min):\", len(log[log['time_diff'] < 60]), \" <> total num:\", log.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (takes ca. 2-4 sec)\n",
    "\n",
    "log = equal_timestamps_interval(log, 60) # 60 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log.to_csv(\"data/combined_event_log-time1m.csv\")\n",
    "log.to_csv(\"data/combined_event_log-filt_evt1p-time1m.csv\")\n",
    "\n",
    "# checkout variant analysis for seeing whether it improved matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_file = \"data/combined_event_log_anonymous.csv\"\n",
    "# src_file = \"data/combined_event_log-filt_evt1p.csv\"\n",
    "# src_file = \"data/combined_event_log-filt_evt1p-time1m.csv\"\n",
    "src_file = \"data/log-filt_unkn-filt_fees.csv\"\n",
    "\n",
    "tgt_folder = \"lifecycles/all-17_10_25/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subprocesses based on status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique activities\n",
    "log['activity'].drop_duplicates().sort_values() #.to_excel(\"data/all_activities-all.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activity subprocesses\n",
    "activ_lifecycles = log[['activity', 'activity_status']].drop_duplicates().sort_values(by=['activity'])\n",
    "activ_lifecycles.to_excel(\"data/activity_lifecycles.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of sub-activities in each subprocess\n",
    "counts = log[['activity', 'activity_status']].drop_duplicates().groupby('activity')['activity_status'].count()\n",
    "counts = counts.sort_values(ascending=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only separate subprocesses with >= 2 sub-activities\n",
    "parent_activ = counts[counts >= 2].reset_index()\n",
    "parent_activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subproc_evts = log[log['activity'].isin(parent_activ['activity'])]\n",
    "subproc_evts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_subproc_evts = log[~ log['activity'].isin(parent_activ['activity'])]\n",
    "non_subproc_evts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create separate logs per subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from separ_subproc import separ_subproc\n",
    "\n",
    "separ_subproc(subproc_evts, non_subproc_evts, 'activity', 'activity_status', 'concept:name', f\"{tgt_folder}/level2/\", f\"{tgt_folder}/combined_event_log-abstracted_status.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subprocesses based on nesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check nesting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_log = pd.read_csv(src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestings[(~ nestings['Activity'].isna()) & (~nestings['Activity'].isin(or_log['activity']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na entries\n",
    "nestings = nestings[~nestings['Activity'].isna()]\n",
    "nestings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstr_log = pd.read_csv(f\"{tgt_folder}/combined_event_log-abstracted_status.csv\")\n",
    "abstr_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect parent items to events\n",
    "# left merge; also keep events that are not being nested\n",
    "abstr_log_parent = abstr_log.merge(nestings, left_on='activity', right_on='Activity', how='left')\n",
    "abstr_log_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subproc_evts = abstr_log_parent[abstr_log_parent['Parent Item'].notna()]\n",
    "# (non-nested events; those not merged with parent)\n",
    "non_subproc_evts = abstr_log_parent[abstr_log_parent['Parent Item'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create separate logs per nested activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from separ_subproc import separ_subproc\n",
    "from shutil import copy\n",
    "import os\n",
    "\n",
    "separ_subproc(subproc_evts, non_subproc_evts, 'Parent Item', 'concept:name', 'concept:name', f\"{tgt_folder}/level1\", f\"{tgt_folder}/combined_event_log-abstracted_nesting.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_folder = f\"{tgt_folder}/level0/logs\"\n",
    "if not os.path.exists(cp_folder):\n",
    "    os.makedirs(cp_folder)\n",
    "copy(f\"{tgt_folder}/combined_event_log-abstracted_nesting.csv\", f\"{cp_folder}/main.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(src_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subprocesses based on status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "found_indexes = set()\n",
    "dir = os.path.join(tgt_folder, \"level2\", \"logs\")\n",
    "sublogs_lvl2 = [ (f, pd.read_csv(os.path.join(dir, f))) for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f)) ]\n",
    "\n",
    "# - find original events in the sublogs (level2, status)\n",
    "# (only level2 events actually exist in original log; level1 is purely for nesting)\n",
    "\n",
    "for f, sublog in sublogs_lvl2:    \n",
    "    f_name = f[0:f.index(\".\")]\n",
    "    \n",
    "    # join original log with the sublog\n",
    "    log_merged = log.merge(sublog, left_on='index', right_on='index')\n",
    "    # print(log_merged)\n",
    "    \n",
    "    # (apply same string operation as on the file name)\n",
    "    activ_file = (log_merged['activity_x'].str.replace(\"/\", \"_\") == f_name)\n",
    "    # for all matches, activity names should correspond to file name\n",
    "    assert (activ_file).all(), f_name + \" <> \" + log_merged[~ activ_file]['activity_x']\n",
    "    # for all matches, activity statuses should correspond to sublog activity name\n",
    "    assert (log_merged['activity_status'] == log_merged['concept:name_y']).all(), \"file: {f_name}\"\n",
    "    \n",
    "    found_indexes.update(log_merged['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - find original log events in the abstracted log (level2, status)\n",
    "\n",
    "abstr_log = pd.read_csv(f\"{tgt_folder}/combined_event_log-abstracted_status.csv\")\n",
    "\n",
    "# only interested in activities without parent (non-nested)\n",
    "log_filter = log[~ log['activity'].isin(parent_activ['activity'])]\n",
    "# join original log with the abstracted log\n",
    "log_merged = log_filter.merge(abstr_log, left_on='index', right_on='index')\n",
    "# print(log_merged)\n",
    "\n",
    "activ_name = (log_merged['concept:name_x'] == log_merged['concept:name_y'])\n",
    "# activity names should correspond\n",
    "assert (activ_name).all(), log_merged[~ activ_name][['concept:name_x', 'concept:name_y']]\n",
    "\n",
    "found_indexes.update(log_merged['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all events found?\n",
    "assert len(found_indexes) == log.shape[0], f\"{len(found_indexes)} <> {log.shape[0]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subprocesses based on nesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per case, check if activity's start/end markers correspond to first & last sorted events of the activity\n",
    "\n",
    "abstr_log = pd.read_csv(f\"{tgt_folder}/combined_event_log-abstracted_status.csv\")\n",
    "\n",
    "def check_subproc(abstr_log, marker_label, is_start):\n",
    "    # group all start/end markers by activity\n",
    "    groups = abstr_log[abstr_log['concept:name'].str.endswith(marker_label)].groupby('activity')\n",
    "\n",
    "    # for each activity & their markers for all cases\n",
    "    for activ, g in groups:\n",
    "        # sort on case\n",
    "        g = g.sort_values(by='case:concept:name').reset_index()\n",
    "        \n",
    "        # find sublog corresponding to activity\n",
    "        for i in [ 1, 2 ]:\n",
    "            path = os.path.join(tgt_folder, f\"level{i}\", \"logs\", f\"{activ.replace('/', '_')}.csv\")\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "            sublog = pd.read_csv(path)\n",
    "        \n",
    "        # print(activ, path)\n",
    "        \n",
    "        # group by case\n",
    "        gb = sublog.groupby('case:concept:name')\n",
    "        # in per-case groups, find the firsts/lasts for each case\n",
    "        delims = (gb.first() if is_start else gb.last()).reset_index()\n",
    "        # also sort on case\n",
    "        delims = delims.sort_values(by='case:concept:name').reset_index()\n",
    "        \n",
    "        # firsts/lasts should be the same as the start/end markers\n",
    "        assert(g['index'] == delims['index']).all(), activ\n",
    "        \n",
    "# for level0: check level1, level2\n",
    "        \n",
    "check_subproc(abstr_log, \" [begin]\", True)\n",
    "check_subproc(abstr_log, \" [end]\", False)\n",
    "\n",
    "# for level 1: check level2\n",
    "\n",
    "path = \"lifecycles/level1/logs/\"\n",
    "\n",
    "from pathlib import Path\n",
    "paths = Path(path).rglob(\"*.csv\")\n",
    "for path in paths:\n",
    "    abstr_sublog = pd.read_csv(path)\n",
    "    check_subproc(abstr_sublog, \" [begin]\", True)\n",
    "    check_subproc(abstr_sublog, \" [end]\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant analysis (bis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main variant (no filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variant_stats import get_variants_stats, get_variant_ratio, get_variant_coverage, get_covering_variants, filter_traces_on_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_log(\"data/combined_event_log-abstracted2.csv\")\n",
    "log = log[['case:concept:name', 'concept:name', 'time:timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_stats = get_variants_stats(log)\n",
    "print(get_variant_ratio(log, var_stats))\n",
    "var_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton_vars = var_stats[var_stats['cov_amt']==1]\n",
    "\n",
    "# ooph, a lot of traces here ...\n",
    "singleton_vars['cov_amt'].sum() / var_stats['cov_amt'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subprocess variants (filtering!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some ad-hoc analysis\n",
    "\n",
    "# log = pd.read_csv(\"/Users/wvw/git/pm/ircc/lifecycles/level1/logs/Other Req Activity.csv\")\n",
    "# var_stats = get_variants_stats(log, plot=False)\n",
    "# print(var_stats)\n",
    "# rare_vars = var_stats[var_stats['cov_perc'] < 1]\n",
    "# print()\n",
    "# print(rare_vars['cov_perc'].sum().round(2))\n",
    "# print((rare_vars['sequence'].count() / var_stats.shape[0] * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the variants\n",
    "# (ca. 3-5 s)\n",
    "\n",
    "dir = \"/Users/wvw/git/pm/ircc/lifecycles\"\n",
    "for subdir in [ \"level1\", \"level2\" ]:\n",
    "    print(\">\", subdir)\n",
    "    for path in Path(os.path.join(dir, subdir, \"logs\")).rglob(\"*.csv\"):\n",
    "        name = os.path.basename(path)\n",
    "        log = pd.read_csv(path)\n",
    "        var_stats = get_variants_stats(log, plot=False)\n",
    "        \n",
    "        rare_vars = var_stats[var_stats['cov_perc'] < 1]\n",
    "        print(name, \":\", rare_vars['cov_perc'].sum().round(2), \"% traces\", \"(\", (rare_vars['sequence'].count() / var_stats.shape[0] * 100).round(2), \"% vars\" \")\")\n",
    "        \n",
    "        flog = filter_traces_on_variants(log, var_stats[var_stats['cov_perc'] >= 1])\n",
    "        flog.to_csv(path)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Timestamp differences (bis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_log(\"data/combined_event_log-abstracted2.csv\")\n",
    "log = log[['case:concept:name', 'concept:name', 'time:timestamp']]\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine_utils import get_time_diff\n",
    "\n",
    "logd = get_time_diff(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (check all sequential events with less than 1min difference in timestamps)\n",
    "print(\"num simult (1 min):\", len(logd[ (logd['time_diff'] > 0) & (logd['time_diff'] < 60)]), \" <> total num:\", logd.shape[0])\n",
    "\n",
    "# none to be found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Infrequent events (bis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"data/combined_event_log-abstracted2.csv\")\n",
    "\n",
    "from log_stats import count_cases_per_event\n",
    "activ_cases_counts = count_cases_per_event('concept:name', 'case:concept:name', log).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activ_cases_counts[activ_cases_counts['perc']<1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine process models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ad-hoc mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variant_stats import get_variants_stats, get_variant_ratio, get_variant_coverage, get_covering_variants, filter_traces_on_variants\n",
    "from mine_utils import get_log, ProcAnn, mine_heur, mine_induct, mine_alpha, mine_dfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_log(\"data/combined_event_log-abstracted2.csv\")\n",
    "\n",
    "cost_recov = log[(log['concept:name']=='Cost Recovery [begin]') | (log['concept:name']=='Cost Recovery [end]')]\n",
    "\n",
    "log = log[['case:concept:name', 'time:timestamp', 'concept:name']]\n",
    "cost_recov.to_csv(\"data/cost_recov.csv\")\n",
    "xes_export.apply(cost_recov, \"data/cost_recov.xes\")\n",
    "\n",
    "mine_heur(log, ProcAnn.FREQ, \"graphs/combined_event_log-abstracted2-time1m\")\n",
    "mine_induct(log, convert_to='petri_net', output_path=\"graphs/combined_event_log-abstracted2-time1m-pn\")\n",
    "\n",
    "log = log[['case:concept:name', 'time:timestamp', 'concept:name']]\n",
    "xes_export.apply(log, \"data/combined_event_log-abstracted2.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif = get_log(\"/Users/wvw/git/pm/ircc/lifecycles/level2/logs/Verification.csv\")\n",
    "verif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = get_variants_stats(verif)\n",
    "vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_dfg(verif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_folder = \"lifecycles/all-17_10_25/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine_utils import ProcAnn, mine_heur, mine_induct, mine_alpha, mine_dfg\n",
    "import pm4py.objects.log.exporter.xes.exporter as xes_export\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "def init_subdir(subdir, subsubdirs=[]):\n",
    "    if os.path.exists(subdir):\n",
    "        shutil.rmtree(subdir)\n",
    "    os.makedirs(subdir)\n",
    "    for subsubdir in subsubdirs:\n",
    "        os.mkdir(os.path.join(subdir, subsubdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "induct_formats = ['bpmn', 'petri_net']\n",
    "formats_with_ann = ['dfg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_entries_json(names, default_format, default_ann, path):\n",
    "    def entry_pref():\n",
    "        if default_format in formats_with_ann:\n",
    "            return f\"{{ \\\"format\\\": \\\"{default_format}\\\", \\\"ann\\\": \\\"{default_ann.value}\\\" }}\"\n",
    "        else:\n",
    "            return f\"{{ \\\"format\\\": \\\"{default_format}\\\" }}\"\n",
    "    \n",
    "    all = \"[\" + \", \".join(map(lambda n: f\"\\\"{n}\\\"\", names)) + \"]\"\n",
    "    prefs = \"{\" + \", \".join(map(lambda n: f\"\\\"{n}\\\": {entry_pref()}\", names)) + \"}\"\n",
    "    obj = f\"{{ \\\"all\\\": {all}, \\\"prefs\\\": {prefs} }}\"\n",
    "    open(os.path.join(path, \"graphs.json\"), \"w\").write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine process models for sublogs\n",
    "# (ca. 1 min)\n",
    "\n",
    "for lvl in [ 1, 2 ]:\n",
    "    # subprocess level (levels 1-2)\n",
    "    subdir = os.path.join(tgt_folder, f\"level{lvl}\")\n",
    "\n",
    "    # default model & annotation to be shown\n",
    "    default_format = \"bpmn\"\n",
    "    default_ann = ProcAnn.FREQ\n",
    "\n",
    "    init_subdir(os.path.join(subdir, \"xes\"))\n",
    "    init_subdir(os.path.join(subdir, \"dfg\"), [ ProcAnn.FREQ.value, ProcAnn.PERF.value ])\n",
    "    init_subdir(os.path.join(subdir, \"heur\"), [ ProcAnn.FREQ.value, ProcAnn.PERF.value ])\n",
    "    for format in induct_formats:\n",
    "        init_subdir(os.path.join(subdir, format))\n",
    "\n",
    "    names = [ ]\n",
    "    for path in Path(os.path.join(tgt_folder, \"data\", f\"level{lvl}\")).rglob(\"*.csv\"):\n",
    "        file = os.path.basename(path)\n",
    "        name = file[0: file.index(\".csv\")]\n",
    "        print(name)\n",
    "        names.append(name)\n",
    "        \n",
    "        log = pd.read_csv(path)\n",
    "        log = log[['case:concept:name', 'time:timestamp', 'concept:name']]\n",
    "        \n",
    "        log['case:concept:name'] = log['case:concept:name'].astype('int64')\n",
    "        log['time:timestamp'] = pd.to_datetime(log['time:timestamp'])\n",
    "        \n",
    "        xes_export.apply(log, os.path.join(subdir, \"xes\", name + \".xes\"))\n",
    "        \n",
    "        for ann in ProcAnn:\n",
    "            mine_dfg(log, ann, output_path=os.path.join(subdir, \"dfg\", ann.value, name), save_gviz=True)\n",
    "            mine_heur(log, ann, output_path=os.path.join(subdir, \"heur\", ann.value, name), save_gviz=True)\n",
    "        \n",
    "        for format in induct_formats:\n",
    "            mine_induct(log, convert_to=format, output_path=os.path.join(subdir, format, name), save_gviz=True)\n",
    "\n",
    "    save_entries_json(names, default_format, default_ann, subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine models for main process\n",
    "# (ca. 30-40 sec)\n",
    "\n",
    "log = pd.read_csv(os.path.join(tgt_folder, \"data\", \"level0\", \"logs\", \"main.csv\"))\n",
    "log = log[['case:concept:name', 'time:timestamp', 'concept:name']]\n",
    "log['time:timestamp'] = pd.to_datetime(log['time:timestamp'])\n",
    "\n",
    "subdir = os.path.join(tgt_folder, \"level0\")\n",
    "\n",
    "default_format = \"dcr\"\n",
    "default_ann = ProcAnn.FREQ\n",
    "\n",
    "init_subdir(os.path.join(subdir, \"logs\"))\n",
    "init_subdir(os.path.join(subdir, \"dfg\"), [ ProcAnn.FREQ.value, ProcAnn.PERF.value ])\n",
    "init_subdir(os.path.join(subdir, \"heur\"), [ ProcAnn.FREQ.value, ProcAnn.PERF.value ])\n",
    "init_subdir(os.path.join(subdir, \"bpmn\"))\n",
    "init_subdir(os.path.join(subdir, \"petri_net\"))\n",
    "init_subdir(os.path.join(subdir, \"xes\"))\n",
    "\n",
    "name = \"main\"\n",
    "log.to_csv(os.path.join(subdir, \"logs\", name + \".csv\"))\n",
    "\n",
    "for ann in ProcAnn:\n",
    "    mine_dfg(log, ann, output_path=os.path.join(subdir, \"dfg\", ann.value, name), save_gviz=True)\n",
    "    mine_heur(log, ann, output_path=os.path.join(subdir, \"heur\", ann.value, name), save_gviz=True)\n",
    "\n",
    "for format in induct_formats:\n",
    "    mine_induct(log, convert_to=format, output_path=os.path.join(subdir, format, name), save_gviz=True)\n",
    "\n",
    "xes_export.apply(log, os.path.join(subdir, \"xes\", name + \".xes\"))\n",
    "\n",
    "save_entries_json([name], default_format, default_ann, subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine DCR: see\n",
    "# /Users/wvw/git/pm/declarative/dcr4py/pm4py-dcr/ircc_dcr.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy files to viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "viewer_folder = f\"viewer/{tgt_folder}\"\n",
    "shutil.rmtree(viewer_folder)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    subf = f\"level{i}\"\n",
    "    shutil.copytree(os.path.join(tgt_folder, subf), os.path.join(viewer_folder, subf))\n",
    "    shutil.rmtree(os.path.join(viewer_folder, subf, \"xes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare lifecycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder1 = \"lifecycles/filt_evt1p/data/level0/logs/main.csv\"\n",
    "folder2 = \"lifecycles/all-17_10_25/data/level0/logs/main.csv\"\n",
    "\n",
    "log1 = pd.read_csv(folder1)\n",
    "log2 = pd.read_csv(folder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Misrep Assessment [begin]', 'Misrep Assessment [end]',\n",
       "       'Biometric Assessment [begin]', 'Other Reqs Assessment [begin]',\n",
       "       'Other Reqs Assessment [end]', 'Biometric Assessment [end]',\n",
       "       'Security Activity [begin]', 'Security Activity [end]',\n",
       "       'Candidate Name Search [begin]', 'Candidate Name Search [end]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activs1 = log1['concept:name']\n",
    "\n",
    "log2[~log2['concept:name'].isin(activs1)]['concept:name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wvw/git/pm/ircc/mine_utils.py:17: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  log = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "from mine_utils import get_log, ProcAnn, mine_heur, mine_induct, mine_alpha, mine_dfg\n",
    "\n",
    "log = get_log(src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = log[(log['activity'] == \"Biometric - FCC - USA\" ) | (log['activity'] == 'Biometric - FCC Detail - USA')]\n",
    "log.to_csv(\"data/Biometric - FCC - USA - all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7733"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log['case:concept:name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_dfg(log, output_path=\"data/Biometric - FCC - USA - all - DFG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_induct(log, convert_to='bpmn', output_path=\"data/Biometric - FCC - USA - all - BPMN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

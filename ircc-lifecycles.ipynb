{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"ircc_uOttawa-filter_evt_10p.csv\")\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_parts = log['event'].apply(lambda e: e.split(\" - \")).to_list()\n",
    "event_parts = pd.DataFrame(event_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event names with more than 2 parts (difficult ones)\n",
    "troublemakers = event_parts[pd.notna(event_parts[2])][0].unique()\n",
    "troublemakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a closer look at the troublemakers\n",
    "# for troublemaker in troublemakers:\n",
    "#     print(event_parts[event_parts[0]==troublemaker].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy ones!\n",
    "activ_lifec = event_parts[~ event_parts[0].isin(troublemakers)]\n",
    "\n",
    "# start from these\n",
    "activ_lifec = pd.DataFrame({ 'activity': activ_lifec[0], 'lifecycle': activ_lifec[1] })\n",
    "# activ_lifec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add activity_cols values to activity; add lifecycle_cols values to lifecycle\n",
    "def create_activity_lifecycle(activity_label, activity_cols, lifecycle_cols, event_parts):\n",
    "    def join_labels(row, cols):\n",
    "        label = \"\"\n",
    "        for col in cols:\n",
    "             label +=  ((\" - \" if label != \"\" else \"\") + row[col] if pd.notna(row[col]) else \"\")\n",
    "        return label\n",
    "    \n",
    "    subset = event_parts[event_parts[0]==activity_label]\n",
    "    activity = subset.apply(lambda row: join_labels(row, activity_cols), axis=1)\n",
    "    lifecycle = subset.apply(lambda row: join_labels(row, lifecycle_cols), axis=1)\n",
    "    \n",
    "    return pd.DataFrame({ 'activity': activity, 'lifecycle': lifecycle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for biographics,\n",
    "# add first two parts to \"activity\"; add last two parts to \"lifecycle\"\n",
    "biometrics_new = create_activity_lifecycle('Biometrics', [0, 1], [2, 3], event_parts)\n",
    "activ_lifec = pd.concat([activ_lifec, biometrics_new])\n",
    "\n",
    "# biometrics_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for biographic,\n",
    "# add first part to \"activity\"; add last three parts to \"lifecycle\"\n",
    "biographic_new = create_activity_lifecycle('Biographic', [0], [1, 2, 3], event_parts)\n",
    "activ_lifec = pd.concat([activ_lifec, biographic_new])\n",
    "\n",
    "# biographic_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for biometric,\n",
    "# add first three parts to \"activity\"; add last two parts to \"lifecycle\"\n",
    "biometric_new = create_activity_lifecycle('Biometric', [0, 1, 2], [3, 4], event_parts)\n",
    "activ_lifec = pd.concat([activ_lifec, biometric_new])\n",
    "\n",
    "# biometric_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the others, \n",
    "for idx in range(3, len(troublemakers)):\n",
    "    troublemaker = troublemakers[idx]\n",
    "    \n",
    "    # add first part to \"activity\"; add last three parts to \"lifecycle\"\n",
    "    activity_new = create_activity_lifecycle(troublemaker, [0], [1, 2, 3], event_parts)\n",
    "    activ_lifec = pd.concat([activ_lifec, activity_new])\n",
    "    \n",
    "    # print(activity_new.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all 'NIL' lifecycle events\n",
    "activ_lifec = activ_lifec[activ_lifec['lifecycle']!='NIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a quick look\n",
    "activ_lifec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique activities\n",
    "len(activ_lifec['activity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the \"activity\" & \"lifecycle\" to original log\n",
    "ext_log = log.join(activ_lifec, how='inner') # merge on shared index\n",
    "\n",
    "# do some checks\n",
    "# activ_lifec should be same size as original log (not if we filtered activ_lifec)\n",
    "# print(activ_lifec.shape[0] == log.shape[0])\n",
    "# join result should have same size as activ_lifec\n",
    "print(activ_lifec.shape[0] == ext_log.shape[0])\n",
    "# event should always start with \"activity\"\n",
    "check1 = ext_log.apply(lambda row: row['event'].startswith(row['activity']), axis=1)\n",
    "print(ext_log[~ check1].shape[0]==0)\n",
    "# event should always include \"lifecycle\"\n",
    "check2 = ext_log.apply(lambda row: row['lifecycle'] in row['event'], axis=1)\n",
    "print(ext_log[~ check2].shape[0]==0)\n",
    "\n",
    "# prepare for pm\n",
    "ext_log = ext_log[['case_id', 'timestamp', 'activity', 'lifecycle']]\n",
    "ext_log = ext_log.rename(columns={ 'case_id': 'case:concept:name', 'lifecycle': 'concept:name', 'timestamp': 'time:timestamp' })\n",
    "ext_log['case:concept:name'] = ext_log['case:concept:name'].astype(str)\n",
    "ext_log['time:timestamp'] = pd.to_datetime(ext_log['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to have a wee look\n",
    "# pd.options.display.max_rows = 100\n",
    "# ext_log[ext_log['case:concept:name']==\"1\"].sort_values(by='time:timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate each activity & its lifecycle into a separate log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but, only do this for activities with >= 3 lifecycle events\n",
    "activ_lifec_counts = activ_lifec[['activity', 'lifecycle']].drop_duplicates().groupby('activity').count().reset_index().sort_values(by='lifecycle')\n",
    "activ_lifec_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ct'ed)\n",
    "separ_activ = activ_lifec_counts[activ_lifec_counts['lifecycle']>=3]['activity']\n",
    "separ_activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_separ = ext_log[ext_log['activity'].isin(separ_activ)]\n",
    "to_separ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_logs = [ (g, df) for g, df in to_separ.groupby('activity') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mine_utils import mine_dfg, mine_alpha, mine_heur, mine_induct\n",
    "# import os, shutil\n",
    "\n",
    "# subdir=\"filter_evt_10p/\"\n",
    "\n",
    "# shutil.rmtree(f\"lifecycles/{subdir}\")\n",
    "# os.mkdir(f\"lifecycles/{subdir}\")\n",
    "# os.mkdir(f\"lifecycles/{subdir}logs/\")\n",
    "# os.mkdir(f\"lifecycles/{subdir}dfg/\")\n",
    "# os.mkdir(f\"lifecycles/{subdir}alpha/\")\n",
    "# os.mkdir(f\"lifecycles/{subdir}heur/\")\n",
    "# os.mkdir(f\"lifecycles/{subdir}induct/\")\n",
    "\n",
    "# # per activity,\n",
    "# for label, sublog in labeled_logs:\n",
    "#     print(f\"{label} (# events: {sublog.shape[0]})\")\n",
    "\n",
    "#     # store log\n",
    "#     sublog.to_csv(f\"lifecycles/{subdir}/logs/{label.replace('/', '_')}\")\n",
    "    \n",
    "#     # mine process model\n",
    "#     mine_dfg(sublog, f\"lifecycles/{subdir}/dfg/{label.replace('/', '_')}\")\n",
    "#     mine_alpha(sublog, f\"lifecycles/{subdir}/alpha/{label.replace('/', '_')}\")\n",
    "#     mine_heur(sublog, f\"lifecycles/{subdir}/heur/{label.replace('/', '_')}\")\n",
    "#     mine_induct(sublog, f\"lifecycles/{subdir}/induct/{label.replace('/', '_')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per case, for the subprocesses, replace all activity lifecycle events by single start & end event\n",
    "\n",
    "sorted_grouped = to_separ.sort_values(['case:concept:name', 'activity', 'time:timestamp']).groupby(['case:concept:name', 'activity'])\n",
    "start_evts = sorted_grouped.first().reset_index(); start_evts['concept:name'] = start_evts['activity'] + ':start'\n",
    "end_evts = sorted_grouped.last().reset_index(); end_evts['concept:name'] = end_evts['activity'] + ':end'\n",
    "abstract_log = pd.concat([start_evts, end_evts])\n",
    "# abstract_log = end_evts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-add the non-subprocess activities\n",
    "\n",
    "not_separ = ext_log[~ext_log['activity'].isin(separ_activ)]\n",
    "not_separ['concept:name'] = not_separ['activity'] + \":\" + not_separ['concept:name']\n",
    "abstract_log = pd.concat([ abstract_log, not_separ ], ignore_index=True).sort_values(by=['case:concept:name','time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'duplicate' events, i.e., same events in a trace occurring within x seconds of each other\n",
    "\n",
    "# NOTE assumes that timestamps are sorted within each group\n",
    "diff = abstract_log.groupby(['case:concept:name', 'concept:name'])['time:timestamp'].diff().astype(int)\n",
    "abstract_log['diff'] = diff\n",
    "abstract_log[abstract_log['diff']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate events with time difference less than max_diff\n",
    "max_diff = 10\n",
    "billion=pow(10,9) # get diffs in seconds\n",
    "to_drop = abstract_log[(abstract_log['diff']>0) & (abstract_log['diff']<max_diff*billion)].index\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_log2 = abstract_log[~abstract_log.index.isin(to_drop)]\n",
    "abstract_log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_log2 = abstract_log2[['case:concept:name', 'concept:name', 'time:timestamp']]\n",
    "\n",
    "# abstract_log2[abstract_log2['case:concept:name']==\"1\"].sort_values(by='time:timestamp')\n",
    "abstract_log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_log2.to_csv(\"abstract_log-starts_ends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "# we not differentiate b/w small subprocesses & non-subprocess events\n",
    "# we simply leave \"small\" subprocesses (2 evts or less) in there\n",
    "# some groups of starts / ends always occur together\n",
    "# seems second/... starts are subprocesses of first/... starts\n",
    "\n",
    "# √ aggregate events where order does not matter\n",
    "# *Fee:outstanding ; *Fee:paid/exempt\n",
    "# replace with singular event when all or subset of group events have occurred\n",
    "\n",
    "# to automatically detect these two cases; \n",
    "# detect tandem pairs (but, what if events interleave; allow noise?)\n",
    "# to detect tandem pairs; use dcr/decl?\n",
    "# WASTE OF TIME\n",
    "# these events have the exact same timestamp\n",
    "# per case, find events with exact same timestamp\n",
    "\n",
    "# check time durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cope with timestamps that are a few seconds from each other)\n",
    "\n",
    "# diff = abstract_log2.groupby(['case:concept:name'])['time:timestamp'].diff().astype(int)\n",
    "# abstract_log2['diff'] = diff\n",
    "# abstract_log2[abstract_log2['diff']>0]\n",
    "\n",
    "# max_diff = 10\n",
    "# billion=pow(10,9) # get diffs in seconds\n",
    "# to_drop = abstract_log[(abstract_log['diff']>0) & (abstract_log['diff']<max_diff*billion)].index\n",
    "# to_drop\n",
    "\n",
    "# abstract_log2 = abstract_log[~abstract_log.index.isin(to_drop)]\n",
    "# abstract_log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine_utils import aggregate_events_replace_last\n",
    "\n",
    "abstract_log3 = abstract_log2.copy() #abstract_log2[abstract_log2['case:concept:name'].isin([\"1\",\"2\",\"3\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activs = abstract_log3['concept:name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees_out = [ a for a in activs if \"Outstanding\" in a ]\n",
    "print(fees_out)\n",
    "abstract_log3 = aggregate_events_replace_last(abstract_log3, fees_out, \"Outstanding:last\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees_paid_exm = [ a for a in activs if \"Paid\" in a or \"Exempt\" in a ]\n",
    "print(fees_paid_exm)\n",
    "abstract_log3 = aggregate_events_replace_last(abstract_log3, fees_paid_exm, \"Paid_Exm:last\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a wee look\n",
    "abstract_log3[abstract_log3['case:concept:name'].isin([\"3\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size0 = abstract_log2.shape[0]\n",
    "size1 = size0 - ((2.352875040154192 - 1) * 12452) - ((2.3639868744872845 - 1) * 12190)\n",
    "\n",
    "print(\"expected:\", size1)\n",
    "print(\"actual:\", abstract_log3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from log_stats import get_trace_lengths\n",
    "get_trace_lengths('concept:name', 'case:concept:name', abstract_log3, plot=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abstract_log3['case:concept:name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variant_stats import get_variants_stats, get_variant_coverage, get_covering_variants, filter_traces_on_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_stats = get_variants_stats(abstract_log3)\n",
    "var_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = get_covering_variants(75, var_stats)\n",
    "vars\n",
    "# vars = var_stats[var_stats['cov_perc']>=1]\n",
    "# vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on 0.1% cov_perc: 14% remaining\n",
    "# filter on 1% cov perc: 48% remaining\n",
    "\n",
    "print(\"remaining coverage:\", vars['cov_perc'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_log4 = filter_traces_on_variants(abstract_log3, vars)\n",
    "abstract_log4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_log4.to_csv(\"abstract_log-starts_ends-cov_var_75perc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py.objects.log.exporter.xes.exporter as xes_export\n",
    "# xes_export.apply(abstract_log4, \"abstract_log-starts_ends-cov_var_75perc.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine_utils import mine_heur\n",
    "\n",
    "mine_heur(abstract_log4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
